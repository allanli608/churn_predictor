{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/content/drive/MyDrive/history.csv\")\n",
    "columns_to_remove = ['id', 'country_code', 'currency_code', 'net_of_fees', 'cashflows_custody_fee', 'fee_paid_separately', 'custody_fee_withdrawal',\n",
    "                     'is_fee_exempt', 'include_client_consolidation', 'credit_limit_type', 'sss_type', 'sss_agent', 'is_midwest_clearing_account',\n",
    "                     'terminal_code', 'target_grantor_grantee_flag', 'iso_funds_code', 'esir_number', 'shareholder_language', 'shareholder_language',\n",
    "                     'conjunction', 'function_code', 'tms_settlement_location', 'portfolio_cost_method', 'portfolio_name_address_option',\n",
    "                     'portfolio_report_option', 'portfolio_summary_option', 'interactive_portfolio_code', 'deceased_fair_market_value', 'rep_commission_rate',\n",
    "                     'dup_trip_quad_code', 'special_fee_code', 'non_calendar_year_end', 'resp_specimen_plan', 'sss_location', 'last_maintenance_user', 'last_maintenance_time',\n",
    "                     'retail_last_maintenance_user', 'retail_last_maintenance_time', 'arp_pension_origin']\n",
    "\n",
    "cols_to_numerify_from_bool = [\"is_registered\", \"is_active\",\n",
    "    \"use_client_address\", \"is_spousal\", \"is_arp_locked\",\n",
    "    \"use_hand_delivery\", \"use_mail\", \"share_name_address_to_issuer\",\n",
    "    \"shareholder_instructions_received\", \"rrsp_limit_reached\", \"is_portfolio_account\",\n",
    "    \"has_no_min_commission\", \"is_tms_eligible\", \"is_agent_bbs_participant\",\n",
    "    \"is_parameters_account\", \"is_spousal_transfer\", \"spousal_age_flag\", \"has_multiple_name\",\n",
    "    \"discretionary_trading_authorized\", \"receive_general_mailings\", \"has_discrete_auth\",\n",
    "    \"is_non_objecting_beneficial_owner\", \"is_objecting_to_disclose_info\", \"consent_to_pay_for_mail\",\n",
    "    \"consent_to_email_delivery\", \"has_received_instruction\", \"is_broker_account\",\n",
    "    \"is_inventory_account\", \"is_gl_account\", \"is_control_account\", \"is_extract_eligible\",\n",
    "    \"is_pledged\", \"is_resp\", \"use_original_date_for_payment_calc\", \"is_family_resp\",\n",
    "    \"is_hrdc_resp\", \"is_plan_grandfathered\", \"is_olob\", \"visible_in_reports\", 'inserted_at', 'updated_at']\n",
    "\n",
    "cols_to_encode = ['class_id', 'type_code', 'debit_code', 'contract_type',\n",
    "                  'branch', 'retail_plan',\n",
    "                  'special_tag', 'guarantee_gtor_type', 'dividend_confirm_code', 'options_trading_type',\n",
    "                  'interest_dividend_conversion_type']\n",
    "\n",
    "cols_null_to_numeric = ['rep_commission_override', 'loan_limit_override', 'non_plan_book_value_flag']\n",
    "\n",
    "cols_date_to_numeric = ['last_trade_date', 'inception_date', 'last_update_date', 'plan_effective_date',\n",
    "                        'plan_end_date', 'rrif_original_date']\n",
    "\n",
    "special_cols = ['language_code', 'title', 'risk_tolerance', 'investment_objective', 'mailing_consent', 'number_of_beneficiaries', 'label']\n",
    "\n",
    "# drop useless columns\n",
    "data = data.drop(columns=columns_to_remove)\n",
    "\n",
    "# store for check later\n",
    "data_columns = data.columns\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of what the dataset looks like\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "counts = data['type_code'].value_counts()\n",
    "\n",
    "# Combine elements with less than 2% occurrence into an 'other' category\n",
    "total_count = counts.sum()\n",
    "counts = counts[counts / total_count >= 0.02]\n",
    "other_count = total_count - counts.sum()\n",
    "counts['Other'] = other_count\n",
    "\n",
    "# Create a pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "plt.title('Unique Account Types Offered')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool to numeric function (cols_to_numerify_from_bool)\n",
    "def bool_to_numeric(df, feature):\n",
    "    bool_to_numeric_dict = {\"t\": 1, \"f\": 0}\n",
    "    df[feature] = df[feature].map(bool_to_numeric_dict).fillna(-1)\n",
    "    return df\n",
    "\n",
    "for col in cols_to_numerify_from_bool:\n",
    "  try:\n",
    "    data = bool_to_numeric(data, col)\n",
    "  except Exception as e:\n",
    "    print(f'Error: {e}')\n",
    "    continue\n",
    "\n",
    "data[cols_to_numerify_from_bool].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode function (cols_to_encode)\n",
    "def encode_and_bind(original_dataframe, feature_to_encode):\n",
    "    temp = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
    "    df = pd.concat([original_dataframe, temp], axis=1)\n",
    "    df = df.drop([feature_to_encode], axis=1)\n",
    "    return(df)\n",
    "\n",
    "for col in cols_to_encode:\n",
    "  try:\n",
    "    data = encode_and_bind(data, col)\n",
    "  except Exception as e:\n",
    "    print(f'Error: {e}')\n",
    "    continue\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for these, null = 0, has value = 1 (cols_null_to_numeric)\n",
    "def null_to_numeric(df, feature):\n",
    "  df[feature] = df[feature].notnull().astype(int)\n",
    "  return df\n",
    "\n",
    "for col in cols_null_to_numeric:\n",
    "  try:\n",
    "    data = null_to_numeric(data, col)\n",
    "  except Exception as e:\n",
    "    print(f'Error: {e}')\n",
    "    continue\n",
    "\n",
    "data[cols_null_to_numeric].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying (cols_date_to_numeric)\n",
    "date_format = \"%Y-%m-%d\"\n",
    "\n",
    "# Assuming `data` is your DataFrame and `date_columns` is a list of date columns\n",
    "for column in cols_date_to_numeric:\n",
    "    try:\n",
    "        # Convert to datetime\n",
    "        data[column] = pd.to_datetime(data[column], errors='coerce')\n",
    "\n",
    "        # Calculate the difference in days from today\n",
    "        data[column] = (datetime.now() - data[column]).dt.days\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        continue\n",
    "\n",
    "# Calculate the average of non-null values for each column\n",
    "column_averages = {column: data[column].dropna().mean() for column in cols_date_to_numeric}\n",
    "\n",
    "# Replace null values with the average of non-null values\n",
    "for column in cols_date_to_numeric:\n",
    "    data[column] = data[column].fillna(column_averages[column])\n",
    "\n",
    "data[cols_date_to_numeric].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIAL COLUMN: language_code\n",
    "# english = 0 , french = 1\n",
    "def update_language(df, feature):\n",
    "  language_to_numeric_dict = {\"F\": 1, \"E\": 0}\n",
    "  df[feature] = df[feature].map(language_to_numeric_dict)\n",
    "  return df\n",
    "\n",
    "data = update_language(data, 'language_code')\n",
    "\n",
    "data['language_code'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIAL COLUMN: title\n",
    "def gender_to_number(df, feature):\n",
    "    mapping = {\n",
    "        'male': ['mr', 'monsieur', 'sir', 'mister'],\n",
    "        'female': ['miss', 'ms', 'mrs', 'madame', 'mme', 'mlle', 'mademoiselle', 'mm']\n",
    "    }\n",
    "    for key in mapping.keys():\n",
    "        mapping[key] = [phrase.lower() for phrase in mapping[key]]\n",
    "\n",
    "    def map_gender(value):\n",
    "        try:\n",
    "            value_lower = str(value).lower()\n",
    "            if pd.isna(value) or value == \"\":\n",
    "                return \"none_provided\"\n",
    "            elif any(phrase in value_lower for phrase in mapping['male']):\n",
    "                return \"male\"\n",
    "            elif any(phrase in value_lower for phrase in mapping['female']):\n",
    "                return \"female\"\n",
    "            else:\n",
    "                return \"other\"\n",
    "        except AttributeError:\n",
    "            return \"none_provided\"\n",
    "    df[feature] = df[feature].apply(map_gender)\n",
    "    df = encode_and_bind(df, feature)\n",
    "    return df\n",
    "\n",
    "data = gender_to_number(data, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIAL COLUMN: risk_tolerance, investment_objective\n",
    "def convert_risk_to_numeric(df, feature):\n",
    "\n",
    "    unique_risks = set()\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        risk_str = row[feature]\n",
    "        risk_str = str(risk_str)\n",
    "        if pd.notna(risk_str):\n",
    "            for char in risk_str:\n",
    "                if char.isalpha():\n",
    "                    unique_risks.add(char)\n",
    "\n",
    "    col_names = [f'{feature}_{risk}' for risk in unique_risks]\n",
    "\n",
    "    for col_name in col_names:\n",
    "        df[col_name] = 0\n",
    "\n",
    "    df[f'{feature}_exists'] = 1\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        risk_str = row[feature]\n",
    "        risk_str = str(risk_str)\n",
    "        if pd.notna(risk_str):\n",
    "            for i in range(len(risk_str)):\n",
    "                if risk_str[i].isalpha():\n",
    "                    col_name = f'{feature}_{risk_str[i]}'\n",
    "                    j = i + 1\n",
    "                    while j < len(risk_str) and risk_str[j].isdigit():\n",
    "                        j += 1\n",
    "                    try:\n",
    "                        value = int(risk_str[i + 1:j]) if j > i + 1 else 0\n",
    "                    except (ValueError, IndexError):\n",
    "                        value = 0\n",
    "                        df.at[idx, f'{feature}_exists'] = 0\n",
    "                    df.at[idx, col_name] = value\n",
    "    df = df.fillna(0)\n",
    "    df.drop(columns=[feature], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "data = convert_risk_to_numeric(data, 'risk_tolerance')\n",
    "data = convert_risk_to_numeric(data, 'investment_objective')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIAL COLUMNS: mailing_consent, number_of_beneficiaries, label\n",
    "data['mailing_consent'] = data['mailing_consent'].fillna(0)\n",
    "data['number_of_beneficiaries'] = data['number_of_beneficiaries'].fillna(0)\n",
    "data['label'] = data['label'].map({'Churn': 1, 'No Churn': 0})\n",
    "\n",
    "# fill all other null cells with -1\n",
    "data = data.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE FINAL DATASET\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "NUMBER_OF_FEATURES_PLS = 50\n",
    "NUMBER_OF_FEATURES_MRMR = 30\n",
    "\n",
    "train_f1 = []\n",
    "test_f1 = []\n",
    "\n",
    "# SPLITTING UP THE DATA INTO INPUT AND OUTPUT\n",
    "X = data.drop(columns=['label'])\n",
    "y = data['label']\n",
    "\n",
    "# SCALE THE DATA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# IMPLEMENT PLS REGRESSION FOR DIMENSIONALITY REDUCTION\n",
    "pls = PLSRegression(n_components=NUMBER_OF_FEATURES_PLS)\n",
    "pls.fit(X_scaled, y)\n",
    "X_pls = pls.transform(X_scaled)\n",
    "\n",
    "# IMPLEMENTING MRMR\n",
    "# Feature Relevance\n",
    "mi_scores = mutual_info_regression(X_pls, y)\n",
    "\n",
    "# Feature Redundancy\n",
    "cos_sim = np.abs(np.corrcoef(X_pls.T))\n",
    "\n",
    "# Calculate MRMR\n",
    "mrmr_scores = mi_scores / np.maximum(np.max(mi_scores) - np.mean(mi_scores), 1e-10)\n",
    "mrmr_scores -= np.mean(cos_sim, axis=1)\n",
    "\n",
    "# Select Features using MRMR\n",
    "selected_features = np.argsort(mrmr_scores)[::-1][:NUMBER_OF_FEATURES_MRMR]\n",
    "\n",
    "\n",
    "selected_feature_names = X.columns[selected_features]\n",
    "print(selected_feature_names)\n",
    "data = data[selected_feature_names]\n",
    "data['label'] = y\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to determine the optimal amount of clusters\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Assuming data is a NumPy array or a pandas DataFrame\n",
    "\n",
    "# Elbow method to determine the optimal number of clusters\n",
    "sse = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 15)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(data, kmeans.labels_))\n",
    "\n",
    "# Plotting Elbow Method\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_range, sse, 'bo-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Sum of Squared Distances (SSE)')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()\n",
    "\n",
    "# Plotting Silhouette Scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_range, silhouette_scores, 'bo-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Scores For Different k')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
